{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection using YOLO Model with Custom Data Training on GPU\n",
    "\n",
    "In this Jupyter Notebook, we will demonstrate how to perform object detection using the YOLO (You Only Look Once) model with custom data training on a GPU. YOLO is a popular real-time object detection algorithm known for its speed and accuracy.\n",
    "\n",
    "### Why YOLO?\n",
    "\n",
    "YOLO is a state-of-the-art deep learning algorithm that can detect multiple objects in an image with a single forward pass. It's known for its real-time processing speed, making it suitable for various applications like self-driving cars, surveillance, and more.\n",
    "\n",
    "### Dataset Preparation\n",
    "\n",
    "Before we proceed with training the YOLO model, we need to prepare our custom dataset. This dataset should include images of objects we want the model to detect and annotations specifying the object's bounding box coordinates and class labels.\n",
    "\n",
    "### YOLO Model Architecture\n",
    "\n",
    "The YOLO model architecture consists of convolutional layers followed by fully connected layers. It divides the input image into a grid and predicts bounding boxes and class probabilities for each grid cell. The YOLO model can efficiently detect objects of different sizes and aspect ratios.\n",
    "\n",
    "### GPU Acceleration\n",
    "\n",
    "Training the YOLO model on large custom datasets can be computationally intensive. To speed up the training process, we will utilize the GPU (Graphics Processing Unit) for parallel processing. GPUs are optimized for handling matrix operations commonly found in deep learning algorithms, making them ideal for accelerating the training process.\n",
    "\n",
    "### Notebook Content\n",
    "\n",
    "1. Dataset Preparation:\n",
    "   - Loading and annotating custom images.\n",
    "   - Organizing the dataset into training and testing sets.\n",
    "\n",
    "2. YOLO Model Configuration:\n",
    "   - Setting up the YOLO model architecture.\n",
    "   - Configuring hyperparameters for training.\n",
    "\n",
    "3. Training the YOLO Model:\n",
    "   - Loading pre-trained weights (optional).\n",
    "   - Training the model on our custom dataset.\n",
    "   - Monitoring training progress and adjusting parameters.\n",
    "\n",
    "4. Object Detection:\n",
    "   - Using the trained YOLO model to detect objects in new images or videos.\n",
    "   - Visualizing the detected objects with bounding boxes and class labels.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "To run this notebook successfully, make sure you have the following installed:\n",
    "\n",
    "- PyTorch \n",
    "- CUDA-compatible GPU and appropriate drivers for GPU acceleration.\n",
    "\n",
    "\n",
    "Let's begin the process of training our YOLO model on custom data using GPU acceleration for efficient and accurate object detection!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 854,
     "status": "ok",
     "timestamp": 1684914168842,
     "user": {
      "displayName": "sukamal das",
      "userId": "02713553165929154470"
     },
     "user_tz": -330
    },
    "id": "GEtWOlXlpspM",
    "outputId": "2c8aa60d-9707-4cf3-81ea-312cd47bc8e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Jul 17 13:15:20 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.199.02   Driver Version: 470.199.02   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla M60           Off  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   38C    P0    38W / 150W |      0MiB /  7618MiB |     61%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing YOLO Class from Ultralytics\n",
    "\n",
    "In this code cell, we are importing the YOLO class from the Ultralytics library. Ultralytics is a powerful library that provides implementations of various state-of-the-art object detection models, including YOLO.\n",
    "\n",
    "The YOLO class in Ultralytics allows us to perform object detection tasks easily with pre-trained models or custom-trained models. It simplifies the process of running object detection on images or videos and provides convenient methods for visualizing the results.\n",
    "\n",
    "To use the YOLO class, make sure you have installed the Ultralytics library and its dependencies. You can install it using pip:\n",
    "\n",
    "```python\n",
    "!pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6224,
     "status": "ok",
     "timestamp": 1684914185640,
     "user": {
      "displayName": "sukamal das",
      "userId": "02713553165929154470"
     },
     "user_tz": -330
    },
    "id": "tSTaymQDqM_F"
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Model Training for Object Detection\n",
    "\n",
    "In this code cell, we are training the YOLO model for object detection using the YOLOv8l architecture on a GPU. We will be training the model for 20 epochs, with an image size of 640x640 pixels and a batch size of 4.\n",
    "\n",
    "### Command Details\n",
    "\n",
    "- `!yolo`: The command to execute the YOLO training script.\n",
    "\n",
    "- `task=detect`: Specifies the task to be performed, which is object detection in this case.\n",
    "\n",
    "- `mode=train`: Specifies the training mode.\n",
    "\n",
    "- `model=yolov8l.pt`: The pre-trained YOLOv8l model to be used as a starting point for training.\n",
    "\n",
    "- `data=/home/ubuntu/environments/model_training_on_GPU/Images/data.yaml`: Path to the data.yaml file, which contains dataset configuration and annotations.\n",
    "\n",
    "- `epochs=20`: The number of epochs to train the model.\n",
    "\n",
    "- `imgsz=640`: The image size used during training (640x640 pixels in this case).\n",
    "\n",
    "- `batch=4`: The batch size for training.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "The YOLO model will be trained on the specified custom dataset with the provided configuration. During training, the model will learn to detect objects in the input images and optimize its parameters to improve detection accuracy.\n",
    "\n",
    "Please ensure that you have set up the necessary dependencies, GPU, and dataset paths correctly before running this code cell. The training process may take some time, depending on the complexity of the dataset and the hardware configuration.\n",
    "\n",
    "**Note**: Replace `/home/ubuntu/environments/model_training_on_GPU/Images/data.yaml` with the actual path to your data.yaml file containing your custom dataset configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 301988,
     "status": "ok",
     "timestamp": 1684914977009,
     "user": {
      "displayName": "sukamal das",
      "userId": "02713553165929154470"
     },
     "user_tz": -330
    },
    "id": "DaGJ9ag4qUOf",
    "outputId": "29b2f0b8-20d4-4a7e-fee2-6c410384c2bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.136 ðŸš€ Python-3.10.6 torch-2.0.1+cu117 CUDA:0 (Tesla M60, 7619MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8l.pt, data=/home/ubuntu/environments/model_training_on_GPU/Images/data.yaml, epochs=20, patience=50, batch=4, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   5585113  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
      "Model summary: 365 layers, 43632153 parameters, 43632137 gradients\n",
      "\n",
      "Transferred 589/595 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/ubuntu/environments/model_training_on_GPU/Images/train/lab\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/ubuntu/environments/model_training_on_GPU/Images/valid/label\u001b[0m\n",
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/20      3.28G      1.328      3.796       1.67          4        640: 1\n",
      "/home/ubuntu/environments/hpepytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13      0.467      0.233      0.174      0.119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/20      3.23G      1.448      4.212      1.815          3        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13      0.527      0.378      0.194      0.122\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/20      3.59G      1.084      4.274       1.66          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13      0.492      0.322      0.193      0.133\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/20      3.62G      1.153      5.286      1.525          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13       0.75        0.4      0.607      0.341\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/20      3.58G      1.153      5.339      1.612          3        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13      0.187      0.533      0.288      0.129\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/20      3.58G       1.49      2.942      1.646          7        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13      0.459      0.533      0.217     0.0782\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/20      3.61G      1.292      4.293       1.59          2        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13     0.0715      0.256      0.102     0.0403\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/20      3.61G      1.211       3.07      1.542          2        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13     0.0715      0.256      0.102     0.0403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/20      3.62G      1.315      2.912      1.603          4        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13      0.223      0.711      0.437      0.121\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/20      3.63G      1.312      2.827      1.685          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13      0.443      0.356     0.0958     0.0367\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/20       3.6G      1.685      3.773      2.131          2        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13      0.443      0.356     0.0958     0.0367\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/20      3.62G      2.121      4.214      2.581          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13     0.0102      0.256    0.00971    0.00272\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/20      3.59G      1.825      3.384      2.156          2        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13     0.0102      0.256    0.00971    0.00272\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/20      3.63G      2.253      3.868      2.507          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13     0.0102      0.256    0.00971    0.00272\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/20      3.64G      2.093      3.618      2.671          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13    0.00566      0.244    0.00506    0.00185\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/20      3.64G        2.2      4.017      2.443          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13    0.00566      0.244    0.00506    0.00185\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/20      3.66G      1.824      3.273       2.24          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13    0.00566      0.244    0.00506    0.00185\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/20      3.66G      1.992      3.579       2.41          2        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13     0.0435      0.378     0.0545     0.0188\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/20      3.64G      1.744      3.095      2.219          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13     0.0435      0.378     0.0545     0.0188\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      20/20      3.65G       1.98      3.305      2.555          1        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13     0.0435      0.378     0.0545     0.0188\n",
      "\n",
      "20 epochs completed in 0.055 hours.\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 87.6MB\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 87.6MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.136 ðŸš€ Python-3.10.6 torch-2.0.1+cu117 CUDA:0 (Tesla M60, 7619MiB)\n",
      "Model summary (fused): 268 layers, 43608921 parameters, 0 gradients\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all          9         13       0.75        0.4        0.6       0.34\n",
      "             softdrink          9          5       0.25        0.2      0.216     0.0238\n",
      "                 chips          9          6          1          0      0.587      0.399\n",
      "               biscuit          9          2          1          1      0.995      0.597\n",
      "Speed: 0.4ms preprocess, 58.9ms inference, 0.0ms loss, 2.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=train model=yolov8l.pt data=/home/ubuntu/environments/model_training_on_GPU/Images/data.yaml epochs=20 imgsz=640 batch=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nVl-xKUXwfbg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPkkG1UHDQNXxNUkob0sqeo",
   "gpuType": "T4",
   "mount_file_id": "1OrD123SMhgSAO99O1PccIHwIFhSUUHUU",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
