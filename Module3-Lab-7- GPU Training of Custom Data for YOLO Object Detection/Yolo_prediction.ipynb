{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f33bd789",
   "metadata": {},
   "source": [
    "# Using the Fine-Tuned YOLO Model for Object Detection on New Data\n",
    "\n",
    "In this Jupyter notebook, we will demonstrate how to use the fine-tuned model of YOLO object detection to predict new data. After training the YOLO model on a custom dataset or pre-trained weights, we can apply the model to detect objects in new images or videos.\n",
    "\n",
    "## Steps to Predict New Data:\n",
    "\n",
    "- Load the Trained Model: Load the YOLO model that has been fine-tuned on the custom dataset. This model should have learned to detect objects relevant to the specific task.\n",
    "\n",
    "- Preprocess the New Data: Before feeding the new data into the model, preprocess it to ensure it is in the correct format and size required by the YOLO model.\n",
    "\n",
    "- Perform Object Detection: Use the loaded fine-tuned YOLO model to predict objects in the new data. The model will identify the objects present in the images or video frames and provide bounding box coordinates and class labels for each detected object.\n",
    "\n",
    "- Visualize the Predictions: Visualize the predictions made by the model by overlaying bounding boxes and class labels on the original images or video frames. This visualization helps in understanding the model's performance and the objects it has detected.\n",
    "\n",
    "## Benefits of Fine-Tuned Models:\n",
    "Fine-tuning a pre-trained YOLO model on a custom dataset can lead to better object detection performance. The model learns to generalize well on the specific objects present in the new dataset, making it more accurate in predicting similar objects in unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ce59530",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = \"pepsi.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16783b6",
   "metadata": {},
   "source": [
    "The following cell \n",
    "1. Importing Libraries:\n",
    "   We first import the necessary libraries to work with YOLO, PyTorch, and OpenCV.\n",
    "\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "2. Loading the Pre-Trained YOLO Model:\n",
    "We load the pre-trained YOLO model from the specified file path. The model is loaded with its weights, architecture, and configuration.\n",
    "\n",
    "3. Performing Object Detection:\n",
    "With the loaded model, we can now perform object detection on an input image. The model's predict method takes an image as input and returns the detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ed9516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/ubuntu/environments/model_training_on_GPU/pepsi.jpg: 640x640 63 softdrinks, 2 chipss, 5 biscuits, 96.1ms\n",
      "Speed: 2.2ms preprocess, 96.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import cv2\n",
    "model = YOLO(\"/home/ubuntu/environments/model_training_on_GPU/runs/detect/train2/weights/best.pt\")\n",
    "results = model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb69d086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'softdrink', 1: 'chips', 2: 'biscuit'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183bc60",
   "metadata": {},
   "source": [
    "## Extracting Results from Object Detection\n",
    "\n",
    "In the previous code cell, we performed object detection using the YOLO model on a given image. Now, let's extract the results and analyze the detected objects.\n",
    "\n",
    "First, we assigned the results of the object detection to the variable `result`. This variable contains information about the detected objects, such as bounding box coordinates, class labels, and confidence scores.\n",
    "\n",
    "Next, we want to find out how many objects were detected in the image. To achieve this, we use the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4c44d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = results[0]\n",
    "len(result.boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f489b95",
   "metadata": {},
   "source": [
    "## Object Detection Results\n",
    "\n",
    "In this code cell, we extract and print the object detection results obtained from the YOLO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38935164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object type: tensor([0.], device='cuda:0')\n",
      "Coordinates: tensor([[ 75.6109, 110.6999, 136.1493, 201.8055]], device='cuda:0')\n",
      "Probability: tensor([1.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "box = result.boxes[0]\n",
    "print(\"Object type:\", box.cls)\n",
    "print(\"Coordinates:\", box.xyxy)\n",
    "print(\"Probability:\", box.conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac153eb2",
   "metadata": {},
   "source": [
    "## Object Detection Results\n",
    "\n",
    "In this code cell, we are printing the results of object detection using the YOLO model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7977965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object type: tensor(0., device='cuda:0')\n",
      "Coordinates: tensor([ 75.6109, 110.6999, 136.1493, 201.8055], device='cuda:0')\n",
      "Probability: tensor(1., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"Object type:\",box.cls[0])\n",
    "print(\"Coordinates:\",box.xyxy[0])\n",
    "print(\"Probability:\",box.conf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c6ac4",
   "metadata": {},
   "source": [
    "## Extracting Object Information\n",
    "\n",
    "In the code cell below, we are extracting information related to detected objects from the `box` variable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f18178fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object type: 0.0\n",
      "Coordinates: [75.61088562011719, 110.6998519897461, 136.1493377685547, 201.80548095703125]\n",
      "Probability: 1.0\n"
     ]
    }
   ],
   "source": [
    "cords = box.xyxy[0].tolist()\n",
    "class_id = box.cls[0].item()\n",
    "conf = box.conf[0].item()\n",
    "print(\"Object type:\", class_id)\n",
    "print(\"Coordinates:\", cords)\n",
    "print(\"Probability:\", conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce276a4",
   "metadata": {},
   "source": [
    "# Object Type and Detection Coordinates\n",
    "\n",
    "In this code cell, we assume that the `class_id` values are floating-point numbers (0.0, 1.0, or 2.0) representing different object types. We have a mapping `object_types` that associates each class_id with a descriptive label.\n",
    "\n",
    "Here's what the code does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b92e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object type: softdrink\n",
      "Coordinates: [76, 111, 136, 202]\n",
      "Probability: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Assuming class_id values are floating-point numbers (0.0, 1.0, or 2.0)\n",
    "object_types = {\n",
    "    0.0: \"softdrink\",\n",
    "    1.0: \"chips\",\n",
    "    2.0: \"biscuit\"\n",
    "}\n",
    "\n",
    "cords = box.xyxy[0].tolist()\n",
    "cords = [round(x) for x in cords]\n",
    "class_id = box.cls[0].item()\n",
    "conf = box.conf[0].item()\n",
    "\n",
    "# Update class_id to the desired object type based on the mapping\n",
    "if class_id in object_types:\n",
    "    class_id = object_types[class_id]\n",
    "else:\n",
    "    class_id = \"Unknown object\"  # Default value if the class_id is not found in the object_types dictionary\n",
    "\n",
    "print(\"Object type:\", class_id)\n",
    "print(\"Coordinates:\", cords)\n",
    "print(\"Probability:\", conf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fd94f",
   "metadata": {},
   "source": [
    "## Object Detection and Bounding Box Visualization\n",
    "\n",
    "In this code cell, we demonstrate how to perform object detection on an input image using the OpenCV library. We draw bounding boxes around the detected objects and display the object type and detection probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4dfc9d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output file is saved to result.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the input image\n",
    "input_image_path = img\n",
    "output_image_path = \"result.jpg\"\n",
    "image = cv2.imread(input_image_path)\n",
    "\n",
    "# Bounding box coordinates\n",
    "\n",
    "# Draw the bounding box on the image\n",
    "start_point = (cords[0], cords[1])\n",
    "end_point = (cords[2], cords[3])\n",
    "color = (0, 255, 0)  # Green color (BGR)\n",
    "thickness = 2\n",
    "image_with_box = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "\n",
    "# Object type and probability\n",
    "class_id = str(class_id)\n",
    "label = f\"{class_id} {conf:.2f}\"\n",
    "\n",
    "# Add the label text near the bounding box\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "font_scale = 0.7\n",
    "font_thickness = 2\n",
    "text_size = cv2.getTextSize(label, font, font_scale, font_thickness)[0]\n",
    "text_coords = (cords[0], cords[1] - 5)\n",
    "image_with_box = cv2.rectangle(image_with_box, text_coords,\n",
    "                               (text_coords[0] + text_size[0], text_coords[1] - text_size[1] - 5), color, -1)\n",
    "cv2.putText(image_with_box, label, text_coords, font, font_scale, (0, 0, 0), font_thickness, cv2.LINE_AA)\n",
    "\n",
    "# Save the result image\n",
    "cv2.imwrite(output_image_path, image_with_box)\n",
    "print(f\"The output file is saved to {output_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76713266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
